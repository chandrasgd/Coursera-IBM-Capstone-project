{
    "cells": [
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n\n<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Objective\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3> \n\n1.  <a href=\"https://#item31\">Import Libraries and Packages</a>\n2.  <a href=\"https://#item32\">Download Data</a>\n3.  <a href=\"https://#item33\">Define Global Constants</a>\n4.  <a href=\"https://#item34\">Construct ImageDataGenerator Instances</a>\n5.  <a href=\"https://#item35\">Compile and Fit Model</a>\n\n</font>\n\n</div>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": ""
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id='item31'></a>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Import Libraries and Packages\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Let's start the lab by importing the libraries that we will be using in this lab.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#pip install keras==2.6.0\n#pip install tensorflow==2.8.0",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "from keras.preprocessing.image import ImageDataGenerator",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "2022-02-20 01:35:37.520775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ibm/dsdriver/lib:/opt/oracle/lib:/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/tensorflow\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip3 install keras-applications\n\n#!pip3 install resnet",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Requirement already satisfied: keras-applications in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (1.0.8)\r\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras-applications) (1.22.2)\r\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from keras-applications) (3.1.0)\r\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "from keras.applications import resnet\nfrom keras.applications.resnet import preprocess_input, decode_predictions\nimport numpy as np",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = resnet.ResNet50\nmodel",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "<function keras.applications.resnet.ResNet50(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000, **kwargs)>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id='item32'></a>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Download Data\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "## get the data\n#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "--2022-02-20 01:35:42--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 261482368 (249M) [application/zip]\nSaving to: \u2018concrete_data_week3.zip.1\u2019\n\nconcrete_data_week3 100%[===================>] 249.37M  37.1MB/s    in 6.9s    \n\n2022-02-20 01:35:49 (36.0 MB/s) - \u2018concrete_data_week3.zip.1\u2019 saved [261482368/261482368]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "cell_type": "code",
            "source": "#!unzip -q concrete_data_week3.zip",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50**\\* error. So please **DO NOT DO IT**.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id='item33'></a>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Define Global Constants\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Here, we will define constants that we will be using throughout the rest of the lab.\n\n1.  We are obviously dealing with two classes, so *num_classes* is 2.\n2.  The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n3.  We will training and validating the model using batches of 100 images.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "num_classes = 2\n\nimage_resize = 224\n\nbatch_size_training = 100\nbatch_size_validation = 100",
            "execution_count": 24,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id='item34'></a>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Construct ImageDataGenerator Instances\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "data_generator = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n)",
            "execution_count": 25,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "train_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/train',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_training,\n    class_mode='categorical')",
            "execution_count": 26,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Found 30001 images belonging to 2 classes.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "## Type your answer here\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode=\"categorical\"\n)",
            "execution_count": 27,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Found 10001 images belonging to 2 classes.\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "train_generator, validation_generator",
            "execution_count": 28,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 28,
                    "data": {
                        "text/plain": "(<keras.preprocessing.image.DirectoryIterator at 0x7f3d6c786af0>,\n <keras.preprocessing.image.DirectoryIterator at 0x7f3d6c39b880>)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Double-click **here** for the solution.\n\n<!-- The correct answer is:\nvalidation_generator = data_generator.flow_from_directory(\n    'concrete_data_week3/valid',\n    target_size=(image_resize, image_resize),\n    batch_size=batch_size_validation,\n    class_mode='categorical')\n-->\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<a id='item35'></a>\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "## Build, Compile and Fit Model\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model = Sequential()",
            "execution_count": 29,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.add(resnet.ResNet50(\n    include_top=False,\n    pooling='avg',\n    weights='imagenet',\n    ))",
            "execution_count": 30,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.add(Dense(num_classes, activation='softmax'))",
            "execution_count": 31,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "You can access the model's layers using the *layers* attribute of our model object.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.layers",
            "execution_count": 32,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 32,
                    "data": {
                        "text/plain": "[<keras.engine.functional.Functional at 0x7f3cc0633b80>,\n <keras.layers.core.Dense at 0x7f3cc03ec5b0>]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "You can access the ResNet50 layers by running the following:\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                },
                "scrolled": true
            },
            "cell_type": "code",
            "source": "model.layers[0].layers",
            "execution_count": 33,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 33,
                    "data": {
                        "text/plain": "[<keras.engine.input_layer.InputLayer at 0x7f3d6c745730>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7f3d6c745040>,\n <keras.layers.convolutional.Conv2D at 0x7f3de83dda00>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3d6c3e26d0>,\n <keras.layers.core.Activation at 0x7f3d200c3520>,\n <keras.layers.convolutional.ZeroPadding2D at 0x7f3d205563a0>,\n <keras.layers.pooling.MaxPooling2D at 0x7f3d6c7a9040>,\n <keras.layers.convolutional.Conv2D at 0x7f3ce062da30>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3de83dd790>,\n <keras.layers.core.Activation at 0x7f3ce0650160>,\n <keras.layers.convolutional.Conv2D at 0x7f3ce0649790>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3ce0650250>,\n <keras.layers.core.Activation at 0x7f3cc0709310>,\n <keras.layers.convolutional.Conv2D at 0x7f3ce063d520>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc07099a0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3ce063dd00>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0711430>,\n <keras.layers.merge.Add at 0x7f3cc071c040>,\n <keras.layers.core.Activation at 0x7f3cc071c1f0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0721040>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc071cf10>,\n <keras.layers.core.Activation at 0x7f3cc0729070>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0721700>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0721e80>,\n <keras.layers.core.Activation at 0x7f3cc06b92e0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06b9e80>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06c10d0>,\n <keras.layers.merge.Add at 0x7f3cc06c6400>,\n <keras.layers.core.Activation at 0x7f3cc06c6fa0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06ccbb0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06c1c70>,\n <keras.layers.core.Activation at 0x7f3cc06d8580>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06d49a0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06d8c10>,\n <keras.layers.core.Activation at 0x7f3cc06e3790>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06e3e20>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06eb8b0>,\n <keras.layers.merge.Add at 0x7f3cc06f6370>,\n <keras.layers.core.Activation at 0x7f3cc06f6970>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0684070>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0684460>,\n <keras.layers.core.Activation at 0x7f3cc0690820>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0690c70>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0690eb0>,\n <keras.layers.core.Activation at 0x7f3cc069b190>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06f2670>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc069e190>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0721160>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06a5b80>,\n <keras.layers.merge.Add at 0x7f3cc069bc70>,\n <keras.layers.core.Activation at 0x7f3cc06883d0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06de5e0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06def70>,\n <keras.layers.core.Activation at 0x7f3cc06c63a0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0729460>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc07350a0>,\n <keras.layers.core.Activation at 0x7f3cc06c13d0>,\n <keras.layers.convolutional.Conv2D at 0x7f3ce0650220>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0701c70>,\n <keras.layers.merge.Add at 0x7f3cc0706610>,\n <keras.layers.core.Activation at 0x7f3cc071beb0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06dc3d0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06dc880>,\n <keras.layers.core.Activation at 0x7f3cc071b7c0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06cddf0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06a9910>,\n <keras.layers.core.Activation at 0x7f3cc071b550>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06a76a0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06b50d0>,\n <keras.layers.merge.Add at 0x7f3cc06af430>,\n <keras.layers.core.Activation at 0x7f3cc06a75e0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc063b940>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc063bcd0>,\n <keras.layers.core.Activation at 0x7f3cc0679130>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc063c3a0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc064c6d0>,\n <keras.layers.core.Activation at 0x7f3cc063c790>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06af1c0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0657640>,\n <keras.layers.merge.Add at 0x7f3cc0654c10>,\n <keras.layers.core.Activation at 0x7f3cc063cf40>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc065f550>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06643a0>,\n <keras.layers.core.Activation at 0x7f3cc06721f0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06648e0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0672760>,\n <keras.layers.core.Activation at 0x7f3cc06061f0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc065fd90>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0606880>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06de9d0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05fb280>,\n <keras.layers.merge.Add at 0x7f3d20419b80>,\n <keras.layers.core.Activation at 0x7f3d20444fa0>,\n <keras.layers.convolutional.Conv2D at 0x7f3d2032af40>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3d203a7f10>,\n <keras.layers.core.Activation at 0x7f3d2005f880>,\n <keras.layers.convolutional.Conv2D at 0x7f3d204530a0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3d20396eb0>,\n <keras.layers.core.Activation at 0x7f3d6c74e400>,\n <keras.layers.convolutional.Conv2D at 0x7f3d006c8a30>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0721e50>,\n <keras.layers.merge.Add at 0x7f3cc06cdf40>,\n <keras.layers.core.Activation at 0x7f3cc0684580>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0654b20>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc063b790>,\n <keras.layers.core.Activation at 0x7f3cc06458e0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0601490>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0606460>,\n <keras.layers.core.Activation at 0x7f3cc05fb610>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06af670>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06a7a90>,\n <keras.layers.merge.Add at 0x7f3d20486d00>,\n <keras.layers.core.Activation at 0x7f3cc06dc670>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06acf10>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06d8c70>,\n <keras.layers.core.Activation at 0x7f3cc071b280>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0729040>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3d20486a00>,\n <keras.layers.core.Activation at 0x7f3cc0701460>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc060b2b0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06270d0>,\n <keras.layers.merge.Add at 0x7f3cc061ecd0>,\n <keras.layers.core.Activation at 0x7f3cc06c1d60>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0630940>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0630cd0>,\n <keras.layers.core.Activation at 0x7f3cc062c100>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06313a0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05c26d0>,\n <keras.layers.core.Activation at 0x7f3cc0631790>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc061eac0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05ce640>,\n <keras.layers.merge.Add at 0x7f3cc05c9a00>,\n <keras.layers.core.Activation at 0x7f3cc0631a60>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc060b1c0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3ce0638490>,\n <keras.layers.core.Activation at 0x7f3cc062c130>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0645850>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0631880>,\n <keras.layers.core.Activation at 0x7f3cc05c90a0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc061e490>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3d203581c0>,\n <keras.layers.merge.Add at 0x7f3d203585e0>,\n <keras.layers.core.Activation at 0x7f3cc0675b50>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06754c0>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc06339a0>,\n <keras.layers.core.Activation at 0x7f3cc05e0cd0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc05e5b80>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05e07f0>,\n <keras.layers.core.Activation at 0x7f3cc05f07c0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc06333a0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc05f0e50>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3d20322d90>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05f7c10>,\n <keras.layers.merge.Add at 0x7f3cc0581370>,\n <keras.layers.core.Activation at 0x7f3cc05819a0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0589670>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc057ed90>,\n <keras.layers.core.Activation at 0x7f3cc0589520>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0581250>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05907f0>,\n <keras.layers.core.Activation at 0x7f3cc05a11f0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc05a1880>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05a7640>,\n <keras.layers.merge.Add at 0x7f3cc0594b20>,\n <keras.layers.core.Activation at 0x7f3cc05aee80>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc053d070>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05a7af0>,\n <keras.layers.core.Activation at 0x7f3cc053d100>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc05b4d00>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc0541fd0>,\n <keras.layers.core.Activation at 0x7f3cc054e0d0>,\n <keras.layers.convolutional.Conv2D at 0x7f3cc0548a00>,\n <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f3cc05a79d0>,\n <keras.layers.merge.Add at 0x7f3cc05891f0>,\n <keras.layers.core.Activation at 0x7f3cc05811c0>,\n <keras.layers.pooling.GlobalAveragePooling2D at 0x7f3cc0589640>]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.layers[0].trainable = False",
            "execution_count": 34,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.summary()",
            "execution_count": 35,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet50 (Functional)        (None, 2048)              23587712  \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 4098      \n=================================================================\nTotal params: 23,591,810\nTrainable params: 4,098\nNon-trainable params: 23,587,712\n_________________________________________________________________\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Next we compile our model using the **adam** optimizer.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])",
            "execution_count": 36,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "steps_per_epoch_training = len(train_generator)\nsteps_per_epoch_validation = len(validation_generator)\nnum_epochs = 2",
            "execution_count": 37,
            "outputs": []
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "#fit_history = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch_training,\n    epochs=num_epochs,\n    validation_data=validation_generator,\n    validation_steps=steps_per_epoch_validation,\n    verbose=1\n)",
            "execution_count": 38,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Epoch 1/2\n301/301 [==============================] - 5920s 20s/step - loss: 0.0381 - accuracy: 0.9846 - val_loss: 0.0086 - val_accuracy: 0.9980\nEpoch 2/2\n301/301 [==============================] - 5700s 19s/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0055 - val_accuracy: 0.9985\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Now that the model is trained, you are ready to start using it to classify images.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "code",
            "source": "model.save('classifier_resnet_model.h5')",
            "execution_count": 39,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  warnings.warn('Custom mask layers require a config and must override '\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "### Thank you for completing this lab!\n\nThis notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3\\_LAB1).\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
        },
        {
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "cell_type": "markdown",
            "source": "<hr>\n\nCopyright \u00a9 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2021-01-01).\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.12",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}